{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa9d44b-7fe4-4e83-b678-f38ff4b0ad7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Preprocessed audio files are saved in 'preprocessed_audios/' directory.\n",
      "DataLoader is ready.\n",
      "WaveNet model is defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import Audio\n",
    "from tqdm.autonotebook import tqdm\n",
    "# Step 1: Preprocessing and Saving Audio Files\n",
    "\n",
    "# Create directory for saving preprocessed audio files\n",
    "os.makedirs('preprocessed_audios', exist_ok=True)\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_and_save_audio(example, idx):\n",
    "    # Load the audio file using librosa from the 'filepath'\n",
    "    y, sr = librosa.load(example['filepath'], sr=None)\n",
    "    \n",
    "    # Convert to mono if it's stereo\n",
    "    y_mono = librosa.to_mono(y)\n",
    "    \n",
    "    # Resample the audio to exactly 22,000 Hz\n",
    "    target_sr = 22000\n",
    "    y_resampled = librosa.resample(y_mono, orig_sr=sr, target_sr=target_sr)\n",
    "    \n",
    "    # Calculate the number of samples for 5 seconds\n",
    "    target_samples = target_sr * 5  # 5 seconds = 110,000 samples\n",
    "\n",
    "    # Trim or pad the audio to exactly 5 seconds (110,000 samples)\n",
    "    y_trimmed = librosa.util.fix_length(y_resampled, size=target_samples)\n",
    "    \n",
    "    # Save the processed audio to a WAV file\n",
    "    filename = f\"preprocessed_audios/{idx}.wav\"\n",
    "    sf.write(filename, y_trimmed, samplerate=target_sr)\n",
    "    \n",
    "    # Optionally, add the path to the saved file to the example dict\n",
    "    example['processed_filepath'] = filename\n",
    "    example['sampling_rate'] = target_sr\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('DBD-research-group/BirdSet', 'HSN', trust_remote_code=True)\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "preprocessed_dataset = dataset['train'].select(range(2000)).map(preprocess_and_save_audio, with_indices=True)\n",
    "\n",
    "print(\"Preprocessing complete. Preprocessed audio files are saved in 'preprocessed_audios/' directory.\")\n",
    "\n",
    "# Step 2: Creating a PyTorch Dataset\n",
    "\n",
    "def mu_law_encoding(x, quantization_channels=256):\n",
    "    mu = quantization_channels - 1\n",
    "    fx = np.sign(x) * np.log1p(mu * np.abs(x)) / np.log1p(mu)\n",
    "    # Map to quantization bins\n",
    "    x_mu = ((fx + 1) / 2 * mu + 0.5).astype(np.int32)\n",
    "    return x_mu\n",
    "\n",
    "class BirdAudioDataset(Dataset):\n",
    "    def __init__(self, file_list, quantization_channels=256):\n",
    "        self.file_list = file_list\n",
    "        self.quantization_channels = quantization_channels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.file_list[idx]\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(filename, sr=None)\n",
    "        # Normalize audio to -1 to 1\n",
    "        y = y / np.abs(y).max()\n",
    "        # Apply mu-law encoding\n",
    "        y_mu = mu_law_encoding(y, self.quantization_channels)\n",
    "        # Convert to torch tensor\n",
    "        y_tensor = torch.from_numpy(y_mu).long()\n",
    "        # Reshape to (1, seq_length)\n",
    "        y_tensor = y_tensor.unsqueeze(0)\n",
    "        return y_tensor\n",
    "\n",
    "# Get the list of preprocessed audio files\n",
    "file_list = glob.glob('preprocessed_audios/*.wav')\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BirdAudioDataset(file_list)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"DataLoader is ready.\")\n",
    "\n",
    "# Step 3: Defining the WaveNet Model\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dilation, residual_channels, skip_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.filter_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=2, dilation=dilation)\n",
    "        self.gate_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=2, dilation=dilation)\n",
    "        self.residual_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=1)\n",
    "        self.skip_conv = nn.Conv1d(residual_channels, skip_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        # Remove padding from input if necessary\n",
    "        padding = (self.filter_conv.kernel_size[0] - 1) * self.filter_conv.dilation[0]\n",
    "        if padding > 0:\n",
    "            x = F.pad(x, (padding, 0))\n",
    "        filter = torch.tanh(self.filter_conv(x))\n",
    "        gate = torch.sigmoid(self.gate_conv(x))\n",
    "        x = filter * gate\n",
    "        skip = self.skip_conv(x)\n",
    "        x = self.residual_conv(x)\n",
    "        # Adjust for size mismatch\n",
    "        x = x[:, :, :residual.size(2)]\n",
    "        x = x + residual\n",
    "        skip = skip[:, :, :residual.size(2)]\n",
    "        return x, skip\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, residual_channels=32, skip_channels=256, quantization_channels=256, dilations=[1, 2, 4, 8, 16, 32]):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=quantization_channels, embedding_dim=residual_channels)\n",
    "        self.residual_blocks = nn.ModuleList(\n",
    "            [ResidualBlock(dilation, residual_channels, skip_channels) for dilation in dilations]\n",
    "        )\n",
    "        self.output_conv1 = nn.Conv1d(skip_channels, skip_channels, kernel_size=1)\n",
    "        self.output_conv2 = nn.Conv1d(skip_channels, quantization_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        skip_connections = []\n",
    "        for block in self.residual_blocks:\n",
    "            x, skip = block(x)\n",
    "            skip_connections.append(skip)\n",
    "        # Make sure all skip connections are the same size\n",
    "        min_length = min([s.size(2) for s in skip_connections])\n",
    "        skip_connections = [s[:, :, :min_length] for s in skip_connections]\n",
    "        x = sum(skip_connections)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.output_conv1(x))\n",
    "        x = self.output_conv2(x)\n",
    "        return x\n",
    "\n",
    "print(\"WaveNet model is defined.\")\n",
    "\n",
    "# Step 4: Training the Model\n",
    "\n",
    "# Initialize model, optimizer, loss function\n",
    "model = WaveNet()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20  # Increase for better results\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        x = batch[:, :, :-1]  # Input sequence\n",
    "        y = batch[:, 0, 1:]   # Target sequence\n",
    "        output = model(x)\n",
    "        # Adjust output and target sizes\n",
    "        min_length = min(output.size(2), y.size(1))\n",
    "        output = output[:, :, :min_length]\n",
    "        y = y[:, :min_length]\n",
    "        output = output.permute(0, 2, 1)  # (batch_size, seq_length, quantization_channels)\n",
    "        output = output.reshape(-1, 256)\n",
    "        y = y.reshape(-1)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Step 5: Generating Synthetic Audio\n",
    "\n",
    "def mu_law_expansion(y_mu, quantization_channels=256):\n",
    "    mu = quantization_channels - 1\n",
    "    y = y_mu.astype(np.float32)\n",
    "    y = y / mu * 2 - 1  # Map values back to [-1, 1]\n",
    "    y = np.sign(y) * (np.exp(np.abs(y) * np.log1p(mu)) - 1) / mu\n",
    "    return y\n",
    "\n",
    "def generate(model, initial_input, num_samples, quantization_channels=256):\n",
    "    model.eval()\n",
    "    generated = []\n",
    "    input_sequence = initial_input  # Should be a tensor of shape (1, 1, seq_length)\n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_sequence)\n",
    "            output = output[:, :, -1]  # Get the last time step\n",
    "            output = F.softmax(output, dim=1)\n",
    "            # Sample from the distribution\n",
    "            distrib = torch.distributions.Categorical(output)\n",
    "            sample = distrib.sample()\n",
    "            generated.append(sample.item())\n",
    "            # Append the new sample to input_sequence\n",
    "            sample = sample.unsqueeze(0).unsqueeze(0)\n",
    "            input_sequence = torch.cat([input_sequence, sample], dim=2)\n",
    "            # Keep the last N samples to avoid memory issues\n",
    "            if input_sequence.size(2) > 1000:\n",
    "                input_sequence = input_sequence[:, :, -1000:]\n",
    "    # Decode mu-law encoding\n",
    "    generated = np.array(generated)\n",
    "    generated = mu_law_expansion(generated, quantization_channels)\n",
    "    return generated\n",
    "\n",
    "# Generate audio\n",
    "initial_input = torch.zeros(1, 1, 1).long()  # Start with silence\n",
    "num_samples = 22000 * 5  # Generate 5 seconds at 22kHz\n",
    "print(\"Generating audio...\")\n",
    "generated_audio = generate(model, initial_input, num_samples)\n",
    "\n",
    "# Save generated audio\n",
    "sf.write('generated_audio.wav', generated_audio, samplerate=22000)\n",
    "print(\"Generated audio saved as 'generated_audio.wav'.\")\n",
    "\n",
    "# Play the generated audio\n",
    "display(Audio(generated_audio, rate=22000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d37429a-1239-46ff-b091-51b006347049",
   "metadata": {},
   "source": [
    "# Why μ-law Encoding\n",
    "\n",
    "μ-law encoding is a nonlinear companding algorithm used in digital telecommunication systems in North America and Japan. It compresses the dynamic range of audio signals before digitization, optimizing signal representation within limited bandwidth.\n",
    "\n",
    "## Some more key points which I found\n",
    "\n",
    "- **Dynamic Range Compression**: Improves the representation of quieter sounds by reducing quantization error at lower amplitudes.\n",
    "- **Logarithmic Companding**: Uses a logarithmic formula to allocate more quantization levels to lower amplitude signals.\n",
    "- **Standard Use**: Commonly used in digital telephony and audio compression systems in specific regions.\n",
    "\n",
    "## Mathematical Formula\n",
    "\n",
    "The μ-law encoding formula is:\n",
    "\n",
    "$$\n",
    "y = \\operatorname{sgn}(x) \\cdot \\frac{\\ln\\left(1 + \\mu |x|\\right)}{\\ln\\left(1 + \\mu\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "- **\\( x \\)**: Normalized input signal (from \\(-1\\) to \\(1\\))\n",
    "- **\\( y \\)**: Compressed output signal\n",
    "- **\\( $\\mu$ \\)**: Compression parameter (typically \\( $\\mu$ = 255 \\))\n",
    "- **\\( $\\operatorname{sgn}(x)$ \\)**: Sign function of \\( x \\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cbf5d-9985-442c-9cf8-b2ec154b126b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
