import librosa
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from datasets import load_dataset
from torch.utils.data import Dataset, DataLoader
from tqdm.autonotebook import tqdm
import soundfile as sf

# Step 1: Preprocessing In-Memory Snippets

def preprocess_audio_in_memory(example, desired_length=1.0, target_sr=22000):
    # Load the audio file using librosa from the 'filepath'
    y, sr = librosa.load(example['filepath'], sr=None)
    duration = librosa.get_duration(y=y, sr=sr)
    
    # Process each detected event to get a snippet of the desired length
    detected_events = example['detected_events']
    if not detected_events:
        return None  # Skip if there are no detected events

    snippets = []
    for event in detected_events:
        if len(event) != 2:
            continue
        start_time, end_time = event
        event_duration = end_time - start_time

        # Adjust the start and end times to get a snippet of the desired length
        if event_duration >= desired_length:
            center_time = (start_time + end_time) / 2
            start_time_adj = max(0, center_time - desired_length / 2)
            end_time_adj = start_time_adj + desired_length
            if end_time_adj > duration:
                end_time_adj = duration
                start_time_adj = end_time_adj - desired_length
        else:
            start_time_adj = max(0, start_time - (desired_length - event_duration) / 2)
            end_time_adj = start_time_adj + desired_length
            if end_time_adj > duration:
                end_time_adj = duration
                start_time_adj = end_time_adj - desired_length
            if start_time_adj < 0:
                start_time_adj = 0
                end_time_adj = desired_length

        # Convert times to sample indices
        start_sample = int(start_time_adj * sr)
        end_sample = int(end_time_adj * sr)

        # Extract the adjusted snippet
        snippet = y[start_sample:end_sample]
        snippet_length = int(desired_length * sr)
        
        # Pad or truncate snippet to desired length
        if len(snippet) < snippet_length:
            padding = snippet_length - len(snippet)
            snippet = np.pad(snippet, (0, padding), 'constant')
        elif len(snippet) > snippet_length:
            snippet = snippet[:snippet_length]

        # Resample the snippet to target_sr
        snippet_resampled = librosa.resample(snippet, orig_sr=sr, target_sr=target_sr)
        snippets.append(snippet_resampled)
    
    return snippets

# Load dataset
print("Loading dataset...")
dataset = load_dataset('DBD-research-group/BirdSet', 'HSN', trust_remote_code=True)
dataset_train = dataset['train']

# Preprocess dataset
print("Preprocessing audio snippets...")
snippets_list = []
for idx, sample in enumerate(dataset_train.select(range(100))):  # Limiting to first 100 samples
    snippets = preprocess_audio_in_memory(sample, desired_length=1.0)  # Adjusted to 1-second snippets
    if snippets:
        snippets_list.extend(snippets)

# Step 2: Creating Dataset for Training

def mu_law_encoding(x, quantization_channels=256):
    mu = quantization_channels - 1
    fx = np.sign(x) * np.log1p(mu * np.abs(x)) / np.log1p(mu)
    x_mu = ((fx + 1) / 2 * mu + 0.5).astype(np.int32)
    return x_mu

class BirdAudioDataset(Dataset):
    def __init__(self, snippets, quantization_channels=256):
        self.snippets = snippets
        self.quantization_channels = quantization_channels
        
    def __len__(self):
        return len(self.snippets)
    
    def __getitem__(self, idx):
        snippet = self.snippets[idx]
        snippet = snippet / np.abs(snippet).max()
        y_mu = mu_law_encoding(snippet, self.quantization_channels)
        y_tensor = torch.from_numpy(y_mu).long().unsqueeze(0)
        return y_tensor

# Create dataset and dataloader
dataset = BirdAudioDataset(snippets_list)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# Step 3: Defining the WaveNet Model

class ResidualBlock(nn.Module):
    def __init__(self, dilation, residual_channels, skip_channels):
        super(ResidualBlock, self).__init__()
        self.filter_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=2, dilation=dilation)
        self.gate_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=2, dilation=dilation)
        self.residual_conv = nn.Conv1d(residual_channels, residual_channels, kernel_size=1)
        self.skip_conv = nn.Conv1d(residual_channels, skip_channels, kernel_size=1)

    def forward(self, x):
        residual = x
        padding = (self.filter_conv.kernel_size[0] - 1) * self.filter_conv.dilation[0]
        if padding > 0:
            x = F.pad(x, (padding, 0))
        filter = torch.tanh(self.filter_conv(x))
        gate = torch.sigmoid(self.gate_conv(x))
        x = filter * gate
        skip = self.skip_conv(x)
        x = self.residual_conv(x)
        x = x[:, :, :residual.size(2)]
        x = x + residual
        skip = skip[:, :, :residual.size(2)]
        return x, skip

class WaveNet(nn.Module):
    def __init__(self, residual_channels=16, skip_channels=64, quantization_channels=256, dilations=[1, 2, 4]):
        super(WaveNet, self).__init__()
        self.embedding = nn.Embedding(num_embeddings=quantization_channels, embedding_dim=residual_channels)
        self.residual_blocks = nn.ModuleList(
            [ResidualBlock(dilation, residual_channels, skip_channels) for dilation in dilations]
        )
        self.output_conv1 = nn.Conv1d(skip_channels, skip_channels, kernel_size=1)
        self.output_conv2 = nn.Conv1d(skip_channels, quantization_channels, kernel_size=1)

    def forward(self, x):
        x = x.squeeze(1)
        x = self.embedding(x)
        x = x.transpose(1, 2)
        skip_connections = []
        for block in self.residual_blocks:
            x, skip = block(x)
            skip_connections.append(skip)
        min_length = min([s.size(2) for s in skip_connections])
        skip_connections = [s[:, :, :min_length] for s in skip_connections]
        x = sum(skip_connections)
        x = F.relu(x)
        x = F.relu(self.output_conv1(x))
        x = self.output_conv2(x)
        return x

# Step 4: Training the Model

model = WaveNet(residual_channels=16, skip_channels=64, dilations=[1, 2, 4])
optimizer = optim.AdamW(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

print("Starting training...")
num_epochs = 2  # Reduced number of epochs
for epoch in range(num_epochs):
    for batch in tqdm(dataloader):
        optimizer.zero_grad()
        x = batch[:, :, :-1]
        y = batch[:, 0, 1:]
        output = model(x)
        min_length = min(output.size(2), y.size(1))
        output = output[:, :, :min_length]
        y = y[:, :min_length]
        output = output.permute(0, 2, 1).reshape(-1, 256)
        y = y.reshape(-1)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}")

# Step 5: Generating Synthetic Audio

def mu_law_expansion(y_mu, quantization_channels=256):
    mu = quantization_channels - 1
    y = y_mu.astype(np.float32)
    y = y / mu * 2 - 1
    y = np.sign(y) * (np.exp(np.abs(y) * np.log1p(mu)) - 1) / mu
    return y

def generate(model, initial_input, num_samples, quantization_channels=256):
    model.eval()
    generated = []
    input_sequence = initial_input
    for _ in tqdm(range(num_samples)):
        with torch.no_grad():
            output = model(input_sequence)
            output = output[:, :, -1]
            output = F.softmax(output, dim=1)
            distrib = torch.distributions.Categorical(output)
            sample = distrib.sample()
            generated.append(sample.item())
            sample = sample.unsqueeze(0).unsqueeze(0)
            input_sequence = torch.cat([input_sequence, sample], dim=2)
            if input_sequence.size(2) > 1000:
                input_sequence = input_sequence[:, :, -1000:]
    generated = np.array(generated)
    return mu_law_expansion(generated, quantization_channels)

initial_input = torch.zeros(1, 1, 1).long()
num_samples = 22000 * 1  # Generating 1 second of audio
print("Generating synthetic audio...")
generated_audio = generate(model, initial_input, num_samples)

# Save the generated audio
sf.write('generated_audio.wav', generated_audio, samplerate=22000)
print("Generated audio saved as 'generated_audio.wav'.")
