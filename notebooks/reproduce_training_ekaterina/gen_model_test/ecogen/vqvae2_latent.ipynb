{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T13:06:45.014855Z",
     "start_time": "2024-10-06T13:05:45.628643Z"
    }
   },
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import numpy as np\n",
    "import lightning as lt\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import soundfile as sf\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import dac\n",
    "from audiotools import AudioSignal\n",
    "import torchaudio\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, n_embeddings, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1 / n_embeddings, 1 / n_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)\n",
    "        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self.embeddings.weight ** 2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self.embeddings.weight.t()))\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        quantized = torch.index_select(self.embeddings.weight, 0, encoding_indices.view(-1))\n",
    "        quantized = quantized.view(inputs.shape)\n",
    "\n",
    "        # Commitment Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "\n",
    "class VQVAE2(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_channels=64, n_embeddings=512, embedding_dim=64):\n",
    "        super(VQVAE2, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, n_embeddings, embedding_dim)\n",
    "        self.vq = VectorQuantizer(n_embeddings, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        x_recon = F.interpolate(x_recon, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        return x_recon, vq_loss\n",
    "\n",
    "\n",
    "\n",
    "class SnippetDatasetHDF(Dataset):\n",
    "    def __init__(self, hdf, scaling='minmax'):\n",
    "        self.num_rows = 0\n",
    "        self.size = int(3.4 * 24000)  # fixed size for samples\n",
    "        self.scaling = scaling\n",
    "        self.data = self.create_data(hdf)\n",
    "        \n",
    "        if scaling == 'standard':\n",
    "            self.mean = self.data.mean()\n",
    "            self.std =  self.data.std()\n",
    "            self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        elif scaling == 'minmax':\n",
    "            self.min = self.data.min()\n",
    "            self.max = self.data.max()\n",
    "            self.data = (self.data - self.min) / (self.max - self.min)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def create_data(self, hdf):\n",
    "        data = []\n",
    "        keys = list(hdf.keys())\n",
    "        self.num_rows = len(keys)\n",
    "        for key in tqdm(keys):\n",
    "            sample = hdf[key]['audio'][:]\n",
    "            if len(sample) > self.size:\n",
    "                self.num_rows -= 1\n",
    "                continue\n",
    "\n",
    "            if len(sample) < self.size:\n",
    "                sample = np.pad(sample, (0, self.size - len(sample)), 'constant')\n",
    "\n",
    "            data.append(sample)\n",
    "         \n",
    "        return torch.tensor(np.array(data)).float()\n",
    "\n",
    "    def retransform(self, data):\n",
    "        if self.scaling == 'standard':\n",
    "            return data * self.std + self.mean\n",
    "        elif self.scaling == 'minmax':\n",
    "            return data * (self.max - self.min) + self.min\n",
    "\n",
    "hdf = h5py.File('../XCM.hdf5', 'r')\n",
    "dataset = SnippetDatasetHDF(hdf)\n",
    "hdf.close()\n",
    "\n",
    "dac_model_path = dac.utils.download(model_type='24kHz')\n",
    "dac_model = dac.DAC.load(dac_model_path)\n",
    "\n",
    "def generate_latents(dataset, model):\n",
    "    latents_list = []\n",
    "    clen = len(dataset)\n",
    "    clen = 2\n",
    "    for i in range(clen):\n",
    "        print(i)\n",
    "        signal = AudioSignal(dataset.retransform(dataset[i]), sample_rate=24000)\n",
    "        wav_dac = model.preprocess(signal.audio_data, signal.sample_rate)\n",
    "        z, codes, latents, _, _ = model.encode(wav_dac)\n",
    "        latents = torch.nn.functional.pad(latents, (0, 1))  \n",
    "        latents_list.append(latents)\n",
    "    \n",
    "    return latents_list\n",
    "\n",
    "latents_list = generate_latents(dataset, dac_model)\n",
    "\n",
    "def train_vqvae2(model, dataloader, epochs=10, lr=1e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for latents in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, vq_loss = model(latents)\n",
    "            loss = criterion(outputs, latents) + vq_loss\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "\n",
    "for idx, latent in enumerate(latents_list):\n",
    "    print(f\"Latent {idx} shape: {latent.shape}\")\n",
    "latents_tensor = torch.stack(latents_list)\n",
    "    \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, latents_tensor):\n",
    "        self.latents_tensor = latents_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.latents_tensor[idx]\n",
    "\n",
    "latent_dataset = LatentDataset(latents_tensor)\n",
    "train_loader = DataLoader(latent_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "vqvae2_model = VQVAE2()\n",
    "train_vqvae2(vqvae2_model, train_loader)\n",
    "\n",
    "def generate_samples(model, num_samples=5):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        encoding_indices = torch.randint(0, model.vq.n_embeddings, (num_samples, 1)).to(next(model.parameters()).device)\n",
    "        quantized_latents = model.vq.embeddings(encoding_indices).view(num_samples, model.vq.embedding_dim, 1, 1)\n",
    "        generated_samples = model.decoder(quantized_latents)\n",
    "        generated_samples = F.interpolate(generated_samples, size=(1, 24000), mode='bilinear', align_corners=False)  # Example size\n",
    "        return generated_samples\n",
    "\n",
    "generated_audio = generate_samples(vqvae2_model, num_samples=5)\n",
    "\n",
    "for i, sample in enumerate(generated_audio):\n",
    "    sample_np = sample.squeeze(0).cpu().numpy()\n",
    "    sample_np = sample_np.squeeze(0)\n",
    "    sf.write(f'generated_sample.wav', sample_np, 24000)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/6675 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df0758c1bbac4763887a35fa82ae2d50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "Latent 0 shape: torch.Size([1, 256, 256])\n",
      "Latent 1 shape: torch.Size([1, 256, 256])\n",
      "Epoch 1/10, Loss: 2.8837718963623047\n",
      "Epoch 2/10, Loss: 2.8790552616119385\n",
      "Epoch 3/10, Loss: 2.8757424354553223\n",
      "Epoch 4/10, Loss: 2.8735742568969727\n",
      "Epoch 5/10, Loss: 2.8723886013031006\n",
      "Epoch 6/10, Loss: 2.8720521926879883\n",
      "Epoch 7/10, Loss: 2.87251353263855\n",
      "Epoch 8/10, Loss: 2.8737995624542236\n",
      "Epoch 9/10, Loss: 2.875980854034424\n",
      "Epoch 10/10, Loss: 2.87921404838562\n"
     ]
    },
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening 'generated_sample.wav': Format not recognised.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLibsndfileError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 224\u001B[0m\n\u001B[0;32m    222\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, sample \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(generated_audio):\n\u001B[0;32m    223\u001B[0m     sample_np \u001B[38;5;241m=\u001B[39m sample\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m--> 224\u001B[0m     \u001B[43msf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgenerated_sample.wav\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_np\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m24000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:343\u001B[0m, in \u001B[0;36mwrite\u001B[1;34m(file, data, samplerate, subtype, endian, format, closefd)\u001B[0m\n\u001B[0;32m    341\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    342\u001B[0m     channels \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m--> 343\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mSoundFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mw\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplerate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    344\u001B[0m \u001B[43m               \u001B[49m\u001B[43msubtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendian\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosefd\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m    345\u001B[0m     f\u001B[38;5;241m.\u001B[39mwrite(data)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:658\u001B[0m, in \u001B[0;36mSoundFile.__init__\u001B[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001B[0m\n\u001B[0;32m    655\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode \u001B[38;5;241m=\u001B[39m mode\n\u001B[0;32m    656\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info \u001B[38;5;241m=\u001B[39m _create_info_struct(file, mode, samplerate, channels,\n\u001B[0;32m    657\u001B[0m                                  \u001B[38;5;28mformat\u001B[39m, subtype, endian)\n\u001B[1;32m--> 658\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_int\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosefd\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    659\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(mode)\u001B[38;5;241m.\u001B[39missuperset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mr+\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseekable():\n\u001B[0;32m    660\u001B[0m     \u001B[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001B[39;00m\n\u001B[0;32m    661\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\soundfile.py:1216\u001B[0m, in \u001B[0;36mSoundFile._open\u001B[1;34m(self, file, mode_int, closefd)\u001B[0m\n\u001B[0;32m   1213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file_ptr \u001B[38;5;241m==\u001B[39m _ffi\u001B[38;5;241m.\u001B[39mNULL:\n\u001B[0;32m   1214\u001B[0m     \u001B[38;5;66;03m# get the actual error code\u001B[39;00m\n\u001B[0;32m   1215\u001B[0m     err \u001B[38;5;241m=\u001B[39m _snd\u001B[38;5;241m.\u001B[39msf_error(file_ptr)\n\u001B[1;32m-> 1216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LibsndfileError(err, prefix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError opening \u001B[39m\u001B[38;5;132;01m{0!r}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname))\n\u001B[0;32m   1217\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode_int \u001B[38;5;241m==\u001B[39m _snd\u001B[38;5;241m.\u001B[39mSFM_WRITE:\n\u001B[0;32m   1218\u001B[0m     \u001B[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001B[39;00m\n\u001B[0;32m   1219\u001B[0m     \u001B[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001B[39;00m\n\u001B[0;32m   1220\u001B[0m     \u001B[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001B[39;00m\n\u001B[0;32m   1221\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mframes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[1;31mLibsndfileError\u001B[0m: Error opening 'generated_sample.wav': Format not recognised."
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54ca44419c03ac1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
