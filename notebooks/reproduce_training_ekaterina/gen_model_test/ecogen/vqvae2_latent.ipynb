{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T15:13:10.578282Z",
     "start_time": "2024-10-06T15:12:43.285875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import numpy as np\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import dac\n",
    "from audiotools import AudioSignal\n",
    "import torchaudio\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, n_embeddings, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1 / n_embeddings, 1 / n_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)\n",
    "        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self.embeddings.weight ** 2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self.embeddings.weight.t()))\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        quantized = torch.index_select(self.embeddings.weight, 0, encoding_indices.view(-1))\n",
    "        quantized = quantized.view(inputs.shape)\n",
    "\n",
    "        # Commitment Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "\n",
    "class VQVAE2(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_channels=64, n_embeddings=512, embedding_dim=64):\n",
    "        super(VQVAE2, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, n_embeddings, embedding_dim)\n",
    "        self.vq = VectorQuantizer(n_embeddings, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        x_recon = F.interpolate(x_recon, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        return x_recon, vq_loss\n",
    "\n",
    "\n",
    "class SnippetDatasetHDF(Dataset):\n",
    "    def __init__(self, hdf, scaling='minmax'):\n",
    "        self.num_rows = 0\n",
    "        self.size = int(3.4 * 24000)  # fixed size for samples\n",
    "        self.scaling = scaling\n",
    "        self.data = self.create_data(hdf)\n",
    "        \n",
    "        if scaling == 'standard':\n",
    "            self.mean = self.data.mean()\n",
    "            self.std =  self.data.std()\n",
    "            self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        elif scaling == 'minmax':\n",
    "            self.min = self.data.min()\n",
    "            self.max = self.data.max()\n",
    "            self.data = (self.data - self.min) / (self.max - self.min)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def create_data(self, hdf):\n",
    "        data = []\n",
    "        keys = list(hdf.keys())\n",
    "        self.num_rows = len(keys)\n",
    "        for key in tqdm(keys):\n",
    "            sample = hdf[key]['audio'][:]\n",
    "            if len(sample) > self.size:\n",
    "                self.num_rows -= 1\n",
    "                continue\n",
    "\n",
    "            if len(sample) < self.size:\n",
    "                sample = np.pad(sample, (0, self.size - len(sample)), 'constant')\n",
    "\n",
    "            data.append(sample)\n",
    "         \n",
    "        return torch.tensor(np.array(data)).float()\n",
    "\n",
    "    def retransform(self, data):\n",
    "        if self.scaling == 'standard':\n",
    "            return data * self.std + self.mean\n",
    "        elif self.scaling == 'minmax':\n",
    "            return data * (self.max - self.min) + self.min\n",
    "\n",
    "def train_vqvae2(model, dataloader, epochs=2, lr=1e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for latents in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, vq_loss = model(latents)\n",
    "            loss = criterion(outputs, latents) + vq_loss\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "\n",
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, latents_tensor):\n",
    "        self.latents_tensor = latents_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.latents_tensor[idx]\n",
    "\n",
    "loaded_data = torch.load('latent_dataset.pt')\n",
    "latent_dataset = LatentDataset(loaded_data)\n",
    "train_loader = DataLoader(latent_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "vqvae2_model = VQVAE2()\n",
    "train_vqvae2(vqvae2_model, train_loader)\n",
    "\n",
    "def generate_samples(model, num_samples=1):\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        encoding_indices = torch.randint(0, model.vq.n_embeddings, (num_samples, 1)).to(next(model.parameters()).device)\n",
    "        quantized_latents = model.vq.embeddings(encoding_indices).view(num_samples, model.vq.embedding_dim, 1, 1)\n",
    "        generated_samples = model.decoder(quantized_latents)\n",
    "        generated_waveform = generated_samples.cpu()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(generated_waveform.squeeze().detach().cpu().numpy(), aspect='auto', origin='lower')\n",
    "        plt.axis('off')\n",
    "        generated_samples = generated_samples.unsqueeze(0) if generated_waveform.ndim == 1 else generated_waveform\n",
    "        return generated_samples\n",
    "\n",
    "generated_audio = generate_samples(vqvae2_model, num_samples=5)\n",
    "\n",
    "for i, sample in enumerate(generated_audio):\n",
    "    sample_np = sample.squeeze(0).cpu().numpy()\n",
    "    print(sample_np)\n",
    "    sample_np = sample_np.astype(np.float32)\n",
    "    if np.max(np.abs(sample_np)) > 1.0:\n",
    "        sample_np /= np.max(np.abs(sample_np))\n",
    "    sample_np_int16 = (sample_np * 32767).astype(np.int16)\n",
    "    torchaudio.save(f'generated_sample_{i}.wav', torch.tensor(sample_np_int16), 24000)\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shale\\AppData\\Local\\Temp\\ipykernel_23080\\1350257228.py:163: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load('latent_dataset.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 2.897915151384142\n",
      "Epoch 2/2, Loss: 2.9157160917917886\n",
      "[[0.5462889  0.541771   0.54097426 0.5442485 ]\n",
      " [0.5502151  0.5431725  0.54663634 0.54986525]\n",
      " [0.5469131  0.53906125 0.54326725 0.54859436]\n",
      " [0.55036694 0.5432585  0.54614216 0.5570344 ]]\n",
      "[[0.5462847  0.54178405 0.5408874  0.5442641 ]\n",
      " [0.55017143 0.5432171  0.54661113 0.549825  ]\n",
      " [0.54696035 0.5391322  0.5432379  0.5486051 ]\n",
      " [0.5503898  0.54320556 0.5461303  0.5570283 ]]\n",
      "[[0.5463347  0.5418717  0.5409209  0.5442656 ]\n",
      " [0.55018526 0.54323506 0.54658824 0.5498141 ]\n",
      " [0.54698616 0.5390901  0.5432423  0.548595  ]\n",
      " [0.5502671  0.54319304 0.5461281  0.557     ]]\n",
      "[[0.54632986 0.5418669  0.54089236 0.544275  ]\n",
      " [0.5501465  0.5430952  0.5466105  0.54982847]\n",
      " [0.54692346 0.5391494  0.5432668  0.5486856 ]\n",
      " [0.5502921  0.54317623 0.54607785 0.557032  ]]\n",
      "[[0.5463431  0.5418266  0.5408919  0.5442332 ]\n",
      " [0.5501736  0.5431339  0.5466335  0.54982656]\n",
      " [0.5469367  0.53910077 0.5432952  0.54863334]\n",
      " [0.5502976  0.54325694 0.5461414  0.55705374]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54ca44419c03ac1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
