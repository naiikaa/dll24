{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T07:11:58.106654Z",
     "start_time": "2024-09-04T07:11:58.009124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "import torchaudio\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from IPython.display import Audio\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "%matplotlib inline \n",
    "\n",
    "class OneHot(nn.Module):\n",
    "    def __init__(self, MU):\n",
    "        super(OneHot, self).__init__()\n",
    "        self.MU = MU\n",
    "        self.ones = torch.sparse.torch.eye(MU).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        batch_size, seq_len = x.size()\n",
    "        x = x.view(-1)\n",
    "\n",
    "        x_one_hot = self.ones.index_select(0, x)\n",
    "        x_one_hot = x_one_hot.view(batch_size, seq_len, self.MU)\n",
    "        x_one_hot = x_one_hot.transpose(1, 2)\n",
    "        return x_one_hot\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"({})\".format(self.MU)\n",
    "\n",
    "\n",
    "class Wavenet(nn.Module):\n",
    "    def __init__(self, dilation_depth, n_blocks, n_dil_channnels, n_residual_channels, n_skip_channels, n_category,\n",
    "                 kernel_size, seq_len_segment):\n",
    "        super(Wavenet, self).__init__()\n",
    "        self.dilation_depth = dilation_depth\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_dil_channnels = n_dil_channnels\n",
    "        self.n_residual_channels = n_residual_channels\n",
    "        self.n_skip_channels = n_skip_channels\n",
    "        self.n_category = n_category\n",
    "        self.kernel_size = kernel_size\n",
    "        self.One_hot = OneHot(n_category)\n",
    "        self.seq_len_segment = seq_len_segment\n",
    "        ###Building the model###\n",
    "        self.dilations = [2 ** i for i in range(dilation_depth)] * n_blocks\n",
    "\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "\n",
    "        ##creating first channels##\n",
    "        self.input_convs = nn.Conv1d(in_channels=self.n_category, out_channels=self.n_residual_channels, kernel_size=1)\n",
    "        ###Creating wavenet blocks stacks###\n",
    "        for d in self.dilations:\n",
    "            self.filter_convs.append(\n",
    "                nn.Conv1d(in_channels=n_residual_channels, out_channels=n_dil_channnels, kernel_size=kernel_size,\n",
    "                          dilation=d))\n",
    "            self.gate_convs.append(\n",
    "                nn.Conv1d(in_channels=n_residual_channels, out_channels=n_dil_channnels, kernel_size=kernel_size,\n",
    "                          dilation=d))\n",
    "            self.residual_convs.append(\n",
    "                nn.Conv1d(in_channels=n_dil_channnels, out_channels=n_residual_channels, kernel_size=1))\n",
    "            self.skip_convs.append(nn.Conv1d(in_channels=n_dil_channnels, out_channels=n_skip_channels, kernel_size=1))\n",
    "        ##post convoluions\n",
    "        self.post_conv1 = nn.Conv1d(in_channels=n_skip_channels, out_channels=n_skip_channels, kernel_size=1)\n",
    "        self.post_conv2 = nn.Conv1d(in_channels=n_skip_channels, out_channels=n_category, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = self.One_hot(x)  # One-hot encoding\n",
    "        x = self.input_convs(x)  # Input convolution\n",
    "        skip_con = 0\n",
    "    \n",
    "        for i in range(self.dilation_depth * self.n_blocks):\n",
    "            dilation = self.dilations[i]\n",
    "            res = x\n",
    "            filter = self.filter_convs[i](x)\n",
    "            filter = torch.tanh(filter)\n",
    "            gate = self.gate_convs[i](x)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            x = filter * gate\n",
    "    \n",
    "            s = self.skip_convs[i](x)\n",
    "            if isinstance(skip_con, int):\n",
    "                skip_con = s\n",
    "            else:\n",
    "                skip_con = skip_con[:, :, -s.size(2):] + s\n",
    "            x = self.residual_convs[i](x)\n",
    "            x = x + res[:, :, dilation * (self.kernel_size - 1):]\n",
    "    \n",
    "        x = torch.relu(skip_con)\n",
    "        x = torch.relu(self.post_conv1(x))\n",
    "        x = self.post_conv2(x)\n",
    "    \n",
    "        # Adjust output length to match the target length\n",
    "        if x.size(2) > self.seq_len_segment:\n",
    "            x = x[:, :, :self.seq_len_segment] \n",
    "        elif x.size(2) < self.seq_len_segment:\n",
    "            padding = self.seq_len_segment - x.size(2)\n",
    "            x = torch.nn.functional.pad(x, (0, padding))  # Pad\n",
    "    \n",
    "        return x\n",
    "\n",
    "    def generate(self, seed_input, num_samples=100):\n",
    "        gen_list = seed_input.squeeze().tolist()\n",
    "        assert len(gen_list) >= sum(self.dilations) + 1, \"Seed input length too short\"\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            if len(gen_list) < sum(self.dilations) + 1:\n",
    "                padding_length = sum(self.dilations) + 1 - len(gen_list)\n",
    "                gen_list = [0] * padding_length + gen_list\n",
    "\n",
    "            x = Variable(torch.LongTensor(gen_list[-sum(self.dilations) - 1:]))\n",
    "            x = x.unsqueeze(0).to(device)  # Add batch dimension\n",
    "            with torch.no_grad():\n",
    "                y = self.forward(x)\n",
    "                y = y.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            _, i = y.max(dim=1)\n",
    "            gen_list.append(i[-1].item())\n",
    "        return gen_list\n",
    "    \n",
    "class BirdsetDataset(Dataset):\n",
    "    def __init__(self, hsn, seq_len_segment, mu):\n",
    "        #self.hsn = hsn\n",
    "        self.hsn = self.createData(hsn)\n",
    "        self.seq_len_segment = seq_len_segment\n",
    "        self.size = 2**15\n",
    "        self.mu = mu\n",
    "        self.data_list = []\n",
    "        for sample in self.hsn:\n",
    "            data, _ = preprocess(sample)\n",
    "            if data.shape[1] >= self.seq_len_segment:\n",
    "                max_val = torch.max(data)\n",
    "                min_val = torch.min(data)\n",
    "                if max_val > torch.abs(min_val):\n",
    "                    data = data / max_val\n",
    "                else:\n",
    "                    data = data / torch.abs(min_val)\n",
    "                self.data_list.append(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "        start = np.random.randint(0, data.shape[1] - self.seq_len_segment)\n",
    "        ys = data[:, start:start + self.seq_len_segment]\n",
    "        ys = mulaw_quantize(ys, self.mu)\n",
    "        ys = ys.squeeze(0)\n",
    "        return ys.to(device)\n",
    "    \n",
    "    def createData(self, hdf):\n",
    "        data = []\n",
    "        keys = list(hdf.keys())\n",
    "        \n",
    "        self.num_rows = len(keys)\n",
    "        for key in tqdm(keys):\n",
    "            sample = hdf[key]['audio'][:]\n",
    "            if len(sample) > self.size:\n",
    "                self.num_rows -= 1\n",
    "                continue\n",
    "\n",
    "            if len(sample) < self.size:\n",
    "                sample = np.pad(sample, (0, self.size - len(sample)), 'constant')\n",
    "\n",
    "            data.append(sample)\n",
    "         \n",
    "        return torch.tensor(np.array(data)).float()\n",
    "    \n",
    "class SnippetDatasetHDF(Dataset):\n",
    "    def __init__(self, hdf, seq_len_segment, mu, scaling='minmax'):\n",
    "        self.num_rows = 0\n",
    "        self.size = 2**15\n",
    "        self.scaling = scaling\n",
    "        self.hsn = self.createData(hsn)\n",
    "        self.seq_len_segment = seq_len_segment\n",
    "        self.size = 2**15\n",
    "        self.mu = mu\n",
    "        self.data = []\n",
    "        \n",
    "        for sample in self.hsn:\n",
    "            data = preprocess(sample)\n",
    "            print(data)\n",
    "            if data.shape[0] >= self.seq_len_segment:\n",
    "                max_val = torch.max(data)\n",
    "                min_val = torch.min(data)\n",
    "                if max_val > torch.abs(min_val):\n",
    "                    data = data / max_val\n",
    "                else:\n",
    "                    data = data / torch.abs(min_val)\n",
    "                self.data.append(data)\n",
    "                \n",
    "        if scaling == 'standard':\n",
    "            self.mean = self.data.mean()\n",
    "            self.std =  self.data.std()\n",
    "            self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        if scaling == 'minmax':\n",
    "            self.min = self.data.min()\n",
    "            self.max = self.data.max()\n",
    "            self.data = (self.data - self.min) / (self.max - self.min)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def createData(self, hdf):\n",
    "        data = []\n",
    "        keys = list(hdf.keys())\n",
    "        self.num_rows = len(keys)\n",
    "        for key in tqdm(keys):\n",
    "            sample = hdf[key]['audio'][:]\n",
    "            if len(sample) > self.size:\n",
    "                self.num_rows -= 1\n",
    "                continue\n",
    "\n",
    "            if len(sample) < self.size:\n",
    "                sample = np.pad(sample, (0, self.size - len(sample)), 'constant')\n",
    "\n",
    "            data.append(sample)\n",
    "         \n",
    "        return torch.tensor(np.array(data)).float()\n",
    "\n",
    "    def retransform(self, data):\n",
    "        if self.scaling == 'standard':\n",
    "            return data * self.std + self.mean\n",
    "        if self.scaling == 'minmax':\n",
    "            return data * (self.max - self.min) + self.min\n",
    "        \n",
    "def mulaw_quantize(x, qc):\n",
    "    assert isinstance(x, torch.Tensor), 'mu_law_encoding expects a Tensor'\n",
    "    mu = qc - 1\n",
    "    if not x.is_floating_point():\n",
    "        x = x.to(torch.float)\n",
    "    mu = torch.tensor(mu, dtype=x.dtype)\n",
    "    x_mu = torch.sign(x) * torch.log1p(mu * torch.abs(x)) / torch.log1p(mu)\n",
    "    x_mu = ((x_mu + 1) / 2 * mu + 0.5).to(torch.int64)\n",
    "    return x_mu\n",
    "\n",
    "\n",
    "\n",
    "def inv_mulaw_quantize(x_mu, quantization_channels=256, device='cpu'):\n",
    "    # Ensure the device is set to the correct value\n",
    "    device = torch.device(device)\n",
    "    mu = quantization_channels - 1.  # Calculate mu\n",
    "    if isinstance(x_mu, np.ndarray):\n",
    "        x = ((x_mu) / mu) * 2 - 1.\n",
    "        x = np.sign(x) * (np.exp(np.abs(x) * np.log1p(mu)) - 1.) / mu\n",
    "    elif isinstance(x_mu, (torch.Tensor, torch.LongTensor)):\n",
    "        if isinstance(x_mu, (torch.LongTensor, torch.cuda.LongTensor)):\n",
    "            x_mu = x_mu.float()\n",
    "        \n",
    "        x_mu = x_mu.to(device)\n",
    "        mu_tensor = torch.FloatTensor([mu]).to(device)\n",
    "        x = ((x_mu) / mu_tensor) * 2 - 1.\n",
    "        x = torch.sign(x) * (torch.exp(torch.abs(x) * torch.log1p(mu_tensor)) - 1.) / mu_tensor\n",
    "    return x\n",
    "\n",
    "\n",
    "def data_generation(data, fr, seq_len_segment, mu, device=device):\n",
    "    max = torch.max(data)\n",
    "    min = torch.min(data)\n",
    "    comparison = max > torch.abs(min)\n",
    "    if torch.all(comparison):\n",
    "        data = torch.div(data, max)\n",
    "    else:\n",
    "        # abs_min_vals = torch.abs(min)\n",
    "        data = torch.div(data, abs(min))\n",
    "    while True:\n",
    "        start = np.random.randint(0, data.shape[1] - seq_len_segment)\n",
    "        ys = data[:, start:start + seq_len_segment]\n",
    "        ys = mulaw_quantize(ys, mu)\n",
    "        ys = ys.squeeze(0)\n",
    "        yield ys.to(device)\n",
    "\n",
    "def preprocess(batch):\n",
    "    print(batch)\n",
    "    # audio, _ = torchaudio.load(batch['filepath'])\n",
    "    if isinstance(batch, torch.Tensor):\n",
    "        audio = batch\n",
    "    else:\n",
    "        audio, _ = torchaudio.load(batch)\n",
    "    if audio.shape[0] > 1:\n",
    "        audio = audio[0] \n",
    "    if len(audio.shape) > 1:\n",
    "        audio = audio.mean(dim=0) \n",
    "    return audio.unsqueeze(0)\n",
    "\n",
    "\n",
    "# hsn = load_dataset('DBD-research-group/BirdSet', 'HSN')\n",
    "hsn = h5py.File('./test_24k.hdf5', 'r')\n",
    "subset_percentage = 0.5\n",
    "seq_len_segment = 40000\n",
    "mu = 128\n",
    "batch_size = 8\n",
    "# dataset = BirdsetDataset(hsn, seq_len_segment, mu)\n",
    "# dataset = SnippetDatasetHDF(hsn)\n",
    "dataset = SnippetDatasetHDF(hsn, seq_len_segment, mu)\n",
    "# hsn.close()\n",
    "#subset_indices = random.sample(range(len(hsn['train'])), int(len(hsn['train']) * subset_percentage))\n",
    "#hsn = hsn['train'].select(subset_indices)\n",
    "display(dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dilation_depth = 10\n",
    "n_blocks = 2\n",
    "n_dilation_channels = 24\n",
    "n_residual_channels = 24\n",
    "n_skip_channels = 128\n",
    "n_category = mu\n",
    "kernel_size = 2\n",
    "model = Wavenet(dilation_depth, n_blocks, n_dilation_channels, n_residual_channels, n_skip_channels, n_category,\n",
    "                kernel_size, seq_len_segment=seq_len_segment)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 35 \n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for i, inputs in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        outputs = outputs.permute(0, 2, 1) \n",
    "        \n",
    "        batch_size, seq_len, num_classes = outputs.size()\n",
    "        outputs = outputs.contiguous().view(-1, num_classes)\n",
    "        targets = inputs.contiguous().view(-1)\n",
    "        \n",
    "        targets = targets.long()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:  \n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {loss.item() / 10:.4f}')\n",
    "\n",
    "\n",
    "print('saving model')\n",
    "torch.save(model.state_dict(), 'wavenet_model.pth')\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/84 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "910efd7b486a4d9793223a2d5e817ee9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.6798e-05, 1.6537e-05, 1.0249e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00])\n",
      "tensor([6.6798e-05])\n",
      "tensor([ 0.0003, -0.0010, -0.0011,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0003])\n",
      "tensor([-0.0014, -0.0027, -0.0002,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0014])\n",
      "tensor([-0.0003, -0.0002,  0.0001,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0003])\n",
      "tensor([-0.0062, -0.0057, -0.0056,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0062])\n",
      "tensor([ 0.0005, -0.0014, -0.0034,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0005])\n",
      "tensor([0.0017, 0.0004, 0.0005,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0017])\n",
      "tensor([0.0033, 0.0004, 0.0014,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0033])\n",
      "tensor([ 0.0001, -0.0002,  0.0013,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0001])\n",
      "tensor([-1.3994e-03, -1.0323e-03,  4.4252e-05,  ...,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([-0.0014])\n",
      "tensor([ 0.0012,  0.0008, -0.0003,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0012])\n",
      "tensor([-3.7931e-04, -7.5577e-05,  1.9747e-04,  ...,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([-0.0004])\n",
      "tensor([0.0012, 0.0054, 0.0045,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0012])\n",
      "tensor([ 0.0007,  0.0002, -0.0005,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0007])\n",
      "tensor([ 0.0007, -0.0009, -0.0014,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0007])\n",
      "tensor([-0.0014, -0.0010, -0.0013,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0014])\n",
      "tensor([-0.0018,  0.0045,  0.0061,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0018])\n",
      "tensor([ 0.0013, -0.0006, -0.0016,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0013])\n",
      "tensor([ 0.0004, -0.0010, -0.0018,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0004])\n",
      "tensor([ 0.0021,  0.0012, -0.0013,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0021])\n",
      "tensor([ 0.0005,  0.0003, -0.0002,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0005])\n",
      "tensor([-0.0007,  0.0003,  0.0004,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0007])\n",
      "tensor([0.0029, 0.0011, 0.0006,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0029])\n",
      "tensor([-0.0003, -0.0016, -0.0026,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0003])\n",
      "tensor([ 0.0010,  0.0027, -0.0003,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0010])\n",
      "tensor([-1.0186e-03,  2.9408e-05,  1.5264e-03,  ...,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([-0.0010])\n",
      "tensor([2.1770e-05, 3.0904e-05, 2.0568e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00])\n",
      "tensor([2.1770e-05])\n",
      "tensor([0.0001, 0.0003, 0.0003,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0001])\n",
      "tensor([ 0.0006,  0.0003, -0.0008,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0006])\n",
      "tensor([0.0011, 0.0007, 0.0016,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0011])\n",
      "tensor([0.0007, 0.0005, 0.0001,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0007])\n",
      "tensor([ 0.0041,  0.0016, -0.0013,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0041])\n",
      "tensor([-3.5910e-03,  1.1330e-03, -1.4588e-05,  ...,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00])\n",
      "tensor([-0.0036])\n",
      "tensor([-0.0013,  0.0018,  0.0010,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0013])\n",
      "tensor([0.0090, 0.0088, 0.0062,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([0.0090])\n",
      "tensor([-0.0024, -0.0033, -0.0005,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.0024])\n",
      "tensor([ 0.0006,  0.0003, -0.0011,  ...,  0.0000,  0.0000,  0.0000])\n",
      "tensor([0.0006])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'min'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 309\u001B[0m\n\u001B[0;32m    306\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;66;03m# dataset = BirdsetDataset(hsn, seq_len_segment, mu)\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;66;03m# dataset = SnippetDatasetHDF(hsn)\u001B[39;00m\n\u001B[1;32m--> 309\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mSnippetDatasetHDF\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhsn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_len_segment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmu\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;66;03m# hsn.close()\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;66;03m#subset_indices = random.sample(range(len(hsn['train'])), int(len(hsn['train']) * subset_percentage))\u001B[39;00m\n\u001B[0;32m    312\u001B[0m \u001B[38;5;66;03m#hsn = hsn['train'].select(subset_indices)\u001B[39;00m\n\u001B[0;32m    313\u001B[0m display(dataset)\n",
      "Cell \u001B[1;32mIn[3], line 208\u001B[0m, in \u001B[0;36mSnippetDatasetHDF.__init__\u001B[1;34m(self, hdf, seq_len_segment, mu, scaling)\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstd\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m scaling \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminmax\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 208\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin\u001B[49m()\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mmax()\n\u001B[0;32m    210\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'min'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "# Load your trained model\n",
    "seq_len_segment = 20000\n",
    "mu = 128\n",
    "batch_size = 8\n",
    "dilation_depth = 10\n",
    "n_blocks = 2\n",
    "n_dilation_channels = 24\n",
    "n_residual_channels = 24\n",
    "n_skip_channels = 128\n",
    "n_category = mu\n",
    "kernel_size = 2\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = Wavenet(dilation_depth, n_blocks, n_dilation_channels, n_residual_channels, n_skip_channels, n_category,\n",
    "                kernel_size, seq_len_segment=seq_len_segment)\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('wavenet_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "def generate_audio(model, seed_length, num_samples, mu, device):\n",
    "    seed_input = torch.randint(0, mu, (1, seed_length)).to(\"cuda\")\n",
    "    generated_sequence = model.generate(seed_input, num_samples=num_samples)\n",
    "    generated_sequence_tensor = torch.LongTensor(generated_sequence).to(\"cuda\")\n",
    "    waveform = inv_mulaw_quantize(generated_sequence_tensor, quantization_channels=mu, device=device)\n",
    "    waveform = waveform.to(device).cpu().numpy()\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "# Parameters\n",
    "seed_length = 10000 \n",
    "num_samples = 10000\n",
    "mu = 128 \n",
    "\n",
    "# Generate audio\n",
    "waveform = generate_audio(model, seed_length, num_samples, mu, device)\n",
    "\n",
    "# Save the generated audio to a file\n",
    "sf.write('generated_audio.wav', waveform, 24000) \n",
    "\n",
    "# Optionally, play the generated audio\n",
    "Audio(waveform, rate=24000)\n"
   ],
   "id": "d7f3c649ce0b237d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7b51f479bb3a92f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "76bc46a0867117f3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
