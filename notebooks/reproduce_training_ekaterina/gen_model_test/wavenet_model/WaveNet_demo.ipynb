{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-05T19:54:41.720951Z",
     "start_time": "2024-09-05T19:40:48.387010Z"
    }
   },
   "source": [
    "import torch\n",
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset\n",
    "from wavenet_training import *\n",
    "from model_logging import *\n",
    "\n",
    "# modified wavenet file implementation from: https://github.com/Vichoko/pytorch-wavenet/tree/master\n",
    "\n",
    "dtype = torch.FloatTensor  # data type\n",
    "ltype = torch.LongTensor  # label type\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    ltype = torch.cuda.LongTensor\n",
    "\n",
    "model = WaveNetModel(layers=10,\n",
    "                     blocks=3,\n",
    "                     dilation_channels=32,\n",
    "                     residual_channels=32,\n",
    "                     skip_channels=1024,\n",
    "                     end_channels=512,\n",
    "                     output_length=16,\n",
    "                     dtype=dtype,\n",
    "                     bias=True)\n",
    "model.cuda()\n",
    "print('model: ', model)\n",
    "print('receptive field: ', model.receptive_field)\n",
    "print('parameter count: ', model.parameter_count())\n",
    "\n",
    "data = WavenetDataset(dataset_file='./example.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      file_location='./unpacked_data',\n",
    "                      test_stride=500)\n",
    "print('the dataset has ' + str(len(data)) + ' items')\n",
    "\n",
    "\n",
    "def generate_and_log_samples(step):\n",
    "    sample_length = 32000\n",
    "    gen_model = load_latest_model_from('snapshots', use_cuda=False)\n",
    "    print(\"start generating...\")\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[0.5])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    # logger.audio_summary('temperature_0.5', tf_samples, step, sr=16000)\n",
    "\n",
    "    samples = generate_audio(gen_model,\n",
    "                             length=sample_length,\n",
    "                             temperatures=[1.])\n",
    "    tf_samples = tf.convert_to_tensor(samples, dtype=tf.float32)\n",
    "    # logger.audio_summary('temperature_1.0', tf_samples, step, sr=16000)\n",
    "    print(\"audio clips generated\")\n",
    "\n",
    "\n",
    "trainer = WavenetTrainer(model=model,\n",
    "                         dataset=data,\n",
    "                         lr=0.001,\n",
    "                         snapshot_path='snapshots',\n",
    "                         snapshot_name='birdset_model',\n",
    "                         snapshot_interval=1000,\n",
    "                         dtype=dtype,\n",
    "                         ltype=ltype)\n",
    "\n",
    "print('start training...')\n",
    "start_data = data[250000][0] \n",
    "display(data)\n",
    "trainer.train(batch_size=16,\n",
    "              epochs=12)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu\n",
      "model:  WaveNetModel(\n",
      "  (filter_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (gate_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(2,), stride=(1,))\n",
      "  )\n",
      "  (residual_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (skip_convs): ModuleList(\n",
      "    (0-29): 30 x Conv1d(32, 1024, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (start_conv): Conv1d(256, 32, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_1): Conv1d(1024, 512, kernel_size=(1,), stride=(1,))\n",
      "  (end_conv_2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "receptive field:  3070\n",
      "parameter count:  1834592\n",
      "one hot input\n",
      "the dataset has 255490 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\pj\\dll24\\notebooks\\reproduce_training_ekaterina\\gen_model_test\\wavenet_model\\wavenet_modules.py:53: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  self.data = Variable(dtype(num_channels, max_length).zero_())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch 0\n",
      "loss at step 50: 3.7523823404312133\n",
      "one training step does take approximately 0.25975210666656495 seconds)\n",
      "loss at step 100: 3.3060987424850463\n",
      "loss at step 150: 3.11121160030365\n",
      "loss at step 200: 3.0084674072265627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.7252301275730133\n",
      "validation accuracy: 20.751953125%\n",
      "loss at step 250: 3.0322255754470824\n",
      "loss at step 300: 2.982674684524536\n",
      "loss at step 350: 2.926995987892151\n",
      "loss at step 400: 2.9541908359527587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.4927340783178806\n",
      "validation accuracy: 26.806640625%\n",
      "loss at step 450: 2.8708009099960328\n",
      "loss at step 500: 2.725304355621338\n",
      "loss at step 550: 2.744828405380249\n",
      "loss at step 600: 2.698204131126404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.28435605019331\n",
      "validation accuracy: 28.06396484375%\n",
      "loss at step 650: 2.5216562509536744\n",
      "loss at step 700: 2.5633893966674806\n",
      "loss at step 750: 2.5318618059158324\n",
      "loss at step 800: 2.411579170227051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.1338761411607265\n",
      "validation accuracy: 30.76171875%\n",
      "loss at step 850: 2.455847804546356\n",
      "loss at step 900: 2.420767467021942\n",
      "loss at step 950: 2.339075019359589\n",
      "loss at step 1000: 2.392481310367584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.075526785105467\n",
      "validation accuracy: 31.23779296875%\n",
      "loss at step 1050: 2.401376276016235\n",
      "loss at step 1100: 2.376470251083374\n",
      "loss at step 1150: 2.4124598360061644\n",
      "loss at step 1200: 2.3052714133262633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.026743870228529\n",
      "validation accuracy: 31.43310546875%\n",
      "loss at step 1250: 2.356872398853302\n",
      "loss at step 1300: 2.3398267674446105\n",
      "loss at step 1350: 2.3631361627578737\n",
      "loss at step 1400: 2.381525673866272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 2.0052985846996307\n",
      "validation accuracy: 31.7138671875%\n",
      "loss at step 1450: 2.325929641723633\n",
      "loss at step 1500: 2.3262641048431396\n",
      "loss at step 1550: 2.3162837624549866\n",
      "loss at step 1600: 2.357054069042206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9523143209517002\n",
      "validation accuracy: 33.70361328125%\n",
      "loss at step 1650: 2.3001373171806336\n",
      "loss at step 1700: 2.278665375709534\n",
      "loss at step 1750: 2.289828083515167\n",
      "loss at step 1800: 2.2546091938018797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9249052926898003\n",
      "validation accuracy: 34.38720703125%\n",
      "loss at step 1850: 2.278935272693634\n",
      "loss at step 1900: 2.3040567779541017\n",
      "loss at step 1950: 2.2493331909179686\n",
      "loss at step 2000: 2.2397066235542296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9045193456113338\n",
      "validation accuracy: 34.86328125%\n",
      "loss at step 2050: 2.260926830768585\n",
      "loss at step 2100: 2.333896429538727\n",
      "loss at step 2150: 2.2659362840652464\n",
      "loss at step 2200: 2.299089729785919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:03<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9239685386419296\n",
      "validation accuracy: 33.31298828125%\n",
      "loss at step 2250: 2.264460108280182\n",
      "loss at step 2300: 2.228082776069641\n",
      "loss at step 2350: 2.2385959792137147\n",
      "loss at step 2400: 2.1913040041923524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.9190063066780567\n",
      "validation accuracy: 33.49609375%\n",
      "loss at step 2450: 2.2558091926574706\n",
      "loss at step 2500: 2.1886008048057555\n",
      "loss at step 2550: 2.204746332168579\n",
      "loss at step 2600: 2.295921447277069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.8833548575639725\n",
      "validation accuracy: 34.53369140625%\n",
      "loss at step 2650: 2.230359013080597\n",
      "loss at step 2700: 2.256378755569458\n",
      "loss at step 2750: 2.1840563154220582\n",
      "loss at step 2800: 2.216467945575714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.8637284226715565\n",
      "validation accuracy: 34.75341796875%\n",
      "loss at step 2850: 2.2385149598121643\n",
      "loss at step 2900: 2.2489780449867247\n",
      "loss at step 2950: 2.2974090433120726\n",
      "loss at step 3000: 2.24168616771698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss: 1.8640907481312752\n",
      "validation accuracy: 34.3994140625%\n",
      "loss at step 3050: 2.172415881156921\n",
      "loss at step 3100: 2.202288272380829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 68\u001B[0m\n\u001B[0;32m     58\u001B[0m trainer \u001B[38;5;241m=\u001B[39m WavenetTrainer(model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m     59\u001B[0m                          dataset\u001B[38;5;241m=\u001B[39mdata,\n\u001B[0;32m     60\u001B[0m                          lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m                          dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[0;32m     65\u001B[0m                          ltype\u001B[38;5;241m=\u001B[39mltype)\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart training...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 68\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m              \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\pj\\dll24\\notebooks\\reproduce_training_ekaterina\\gen_model_test\\wavenet_model\\wavenet_training.py:73\u001B[0m, in \u001B[0;36mWavenetTrainer.train\u001B[1;34m(self, batch_size, epochs, continue_training_at_step)\u001B[0m\n\u001B[0;32m     71\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(output\u001B[38;5;241m.\u001B[39msqueeze(), target\u001B[38;5;241m.\u001B[39msqueeze())\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 73\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    766\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    767\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 768\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    769\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    770\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    771\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    772\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T20:53:33.157400Z",
     "start_time": "2024-09-05T20:46:33.232987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset\n",
    "from wavenet_training import *\n",
    "from model_logging import *\n",
    "dtype = torch.FloatTensor  # data type\n",
    "ltype = torch.LongTensor  # label type\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    ltype = torch.cuda.LongTensor\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = WaveNetModel(layers=10,\n",
    "                     blocks=3,\n",
    "                     dilation_channels=32,\n",
    "                     residual_channels=32,\n",
    "                     skip_channels=1024,\n",
    "                     end_channels=512,\n",
    "                     output_length=16,\n",
    "                     dtype=dtype,\n",
    "                     bias=True)\n",
    "model.load_state_dict(torch.load('birdset_modelwavenet_model.pth', weights_only=False))\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "data = WavenetDataset(dataset_file='./example.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      file_location='./unpacked_data',\n",
    "                      test_stride=500)\n",
    "print('the dataset has ' + str(len(data)) + ' items')\n",
    "\n",
    "\n",
    "\n",
    "start_data = data[250000][0]\n",
    "start_data = torch.max(start_data, 0)[1].to('cuda')\n",
    "def prog_callback(step, total_steps):\n",
    "    print(str(100 * step // total_steps) + \"% generated\")\n",
    "\n",
    "generated = model.generate_fast(num_samples=32000,\n",
    "                                 first_samples=start_data,\n",
    "                                 progress_callback=prog_callback,\n",
    "                                 progress_interval=1000,\n",
    "                                 temperature=1.0,\n",
    "                                 regularize=0.)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu\n",
      "one hot input\n",
      "the dataset has 255490 items\n",
      "torch.Size([1, 256, 1])\n",
      "0% generated\n",
      "2% generated\n",
      "5% generated\n",
      "8% generated\n",
      "one generating step does take approximately 0.011234934329986573 seconds)\n",
      "11% generated\n",
      "14% generated\n",
      "17% generated\n",
      "19% generated\n",
      "22% generated\n",
      "25% generated\n",
      "28% generated\n",
      "31% generated\n",
      "34% generated\n",
      "37% generated\n",
      "39% generated\n",
      "42% generated\n",
      "45% generated\n",
      "48% generated\n",
      "51% generated\n",
      "54% generated\n",
      "57% generated\n",
      "59% generated\n",
      "62% generated\n",
      "65% generated\n",
      "68% generated\n",
      "71% generated\n",
      "74% generated\n",
      "76% generated\n",
      "79% generated\n",
      "82% generated\n",
      "85% generated\n",
      "88% generated\n",
      "91% generated\n",
      "94% generated\n",
      "96% generated\n",
      "99% generated\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T20:56:55.831391Z",
     "start_time": "2024-09-05T20:56:55.825098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import IPython.display as ipd\n",
    "import soundfile as sf\n",
    "\n",
    "print(generated.shape)\n",
    "ipd.Audio(generated, rate=16000)\n",
    "\n",
    "sf.write('output_file.wav', generated, 16000)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000,)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
