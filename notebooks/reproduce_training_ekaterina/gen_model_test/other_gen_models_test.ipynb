{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-09T16:47:47.549747Z",
     "start_time": "2024-08-09T16:47:42.469475Z"
    }
   },
   "source": [
    "import torch\n",
    "from notebooks.reproduce_training_ekaterina.gen_model_test.vqvae2 import VQVAE\n",
    "from IPython.display import Audio\n",
    "from datasets import load_dataset\n",
    "import lightning as lt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import soundfile as sf\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "class Lightningwrapper(lt.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pred, latent_loss = self.model(batch)\n",
    "        recon_loss = F.mse_loss(pred, batch.squeeze())\n",
    "        loss = latent_loss + recon_loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.model.encode(x)\n",
    "\n",
    "    def decode(self, quant_t, quant_b):\n",
    "        return self.model.decode(quant_t, quant_b)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=3e-4)\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataset, key='train'):\n",
    "        self.key = key\n",
    "        self.num_rows = 0\n",
    "        self.target_shape = (1, 1, 64, 64)\n",
    "        self.data = self.createData(dataset)\n",
    "\n",
    "        self.mean = self.data.mean()\n",
    "        self.std = self.data.std()\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "\n",
    "    def retransform(self, data):\n",
    "        if self.scaling == 'standard':\n",
    "            return data * self.std + self.mean\n",
    "        if self.scaling == 'minmax':\n",
    "            return data * (self.max - self.min) + self.min\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def createData(self, dataset):\n",
    "        data = []\n",
    "\n",
    "        for row in tqdm(dataset[self.key]):\n",
    "            if self.num_rows > 100:\n",
    "                break\n",
    "            file_path = row['filepath']\n",
    "            sample, samplerate = sf.read(file_path)\n",
    "            if len(sample.shape) != 1:\n",
    "                sample = sample[:, 0]\n",
    "            if len(sample) < 2 ** 16:\n",
    "                continue\n",
    "            if row['quality'] in ['B', 'C']:\n",
    "                continue\n",
    "\n",
    "            # Convert to tensor\n",
    "            samplex = torch.tensor(sample[:2 ** 16], dtype=torch.float32)\n",
    "            samplex = samplex.squeeze().view(1, 256, 256)\n",
    "            self.num_rows += 1\n",
    "            data.append(samplex)\n",
    "\n",
    "        return torch.stack(data)\n",
    "\n",
    "\n",
    "model = VQVAE(in_channel=1)\n",
    "checkpoint_path = './checkpoints/epoch=20.ckpt'\n",
    "lt_model = Lightningwrapper(model)\n",
    "lt_model.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cpu')))\n",
    "\n",
    "hsn = load_dataset('DBD-research-group/BirdSet', 'HSN')\n",
    "dataset = AudioDataset(hsn)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "lt_model.eval()\n",
    "sample_batch = next(iter(dataloader))\n",
    "sample = sample_batch[0]\n",
    "\n",
    "if sample.dim() == 3:\n",
    "    sample = sample.unsqueeze(0)\n",
    "\n",
    "lt_model.eval()\n",
    "with torch.no_grad():\n",
    "    quant_t, quant_b, _, _, _ = lt_model.encode(sample)\n",
    "print(\"quant_b shape:\", quant_b.shape)\n",
    "print(\"quant_t shape:\", quant_t.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_image = lt_model.decode(quant_t, quant_b)\n",
    "\n",
    "generated_image_np = generated_image[0, 0].cpu().numpy() \n",
    "samplerate = 32000\n",
    "Audio(generated_image_np, rate=samplerate)\n",
    "\n",
    "from scipy.io.wavfile import write\n",
    "write('output.wav', samplerate, generated_image_np)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/5460 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1650e47368a540ec9808162d8869597d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant_b shape: torch.Size([1, 64, 64, 64])\n",
      "quant_t shape: torch.Size([1, 64, 32, 32])\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bde9d465161744f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
