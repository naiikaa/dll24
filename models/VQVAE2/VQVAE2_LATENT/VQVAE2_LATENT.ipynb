{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-12T17:36:39.693603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import numpy as np\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "import dac\n",
    "from audiotools import AudioSignal\n",
    "import torchaudio\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, n_embeddings, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1 / n_embeddings, 1 / n_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)\n",
    "        distances = (torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self.embeddings.weight ** 2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self.embeddings.weight.t()))\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        quantized = torch.index_select(self.embeddings.weight, 0, encoding_indices.view(-1))\n",
    "        quantized = quantized.view(inputs.shape)\n",
    "\n",
    "        # Commitment Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "\n",
    "class VQVAE2(nn.Module):\n",
    "    def __init__(self, in_channels=1, hidden_channels=64, n_embeddings=512, embedding_dim=64):\n",
    "        super(VQVAE2, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, n_embeddings, embedding_dim)\n",
    "        self.vq = VectorQuantizer(n_embeddings, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        x_recon = F.interpolate(x_recon, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        return x_recon, vq_loss\n",
    "\n",
    "\n",
    "class SnippetDatasetHDF(Dataset):\n",
    "    def __init__(self, hdf, scaling='minmax'):\n",
    "        self.num_rows = 0\n",
    "        self.size = int(3.4 * 24000)  # fixed size for samples\n",
    "        self.scaling = scaling\n",
    "        self.data = self.create_data(hdf)\n",
    "\n",
    "        if scaling == 'standard':\n",
    "            self.mean = self.data.mean()\n",
    "            self.std = self.data.std()\n",
    "            self.data = (self.data - self.mean) / self.std\n",
    "\n",
    "        elif scaling == 'minmax':\n",
    "            self.min = self.data.min()\n",
    "            self.max = self.data.max()\n",
    "            self.data = (self.data - self.min) / (self.max - self.min)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_rows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def create_data(self, hdf):\n",
    "        data = []\n",
    "        keys = list(hdf.keys())\n",
    "        self.num_rows = len(keys)\n",
    "        for key in tqdm(keys):\n",
    "            sample = hdf[key]['audio'][:]\n",
    "            if len(sample) > self.size:\n",
    "                self.num_rows -= 1\n",
    "                continue\n",
    "\n",
    "            if len(sample) < self.size:\n",
    "                sample = np.pad(sample, (0, self.size - len(sample)), 'constant')\n",
    "\n",
    "            data.append(sample)\n",
    "\n",
    "        return torch.tensor(np.array(data)).float()\n",
    "\n",
    "    def retransform(self, data):\n",
    "        if self.scaling == 'standard':\n",
    "            return data * self.std + self.mean\n",
    "        elif self.scaling == 'minmax':\n",
    "            return data * (self.max - self.min) + self.min\n",
    "\n",
    "\n",
    "def train_vqvae2(model, dataloader, epochs=15, lr=1e-5):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    #criterion = nn.MSELoss()\n",
    "    criterion = nn.L1Loss()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for latents in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs, vq_loss = model(latents)\n",
    "            loss = criterion(outputs, latents) + vq_loss\n",
    "            loss.backward(retain_graph=True)\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "\n",
    "class LatentDataset(Dataset):\n",
    "    def __init__(self, latents_tensor):\n",
    "        self.latents_tensor = latents_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latents_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.latents_tensor[idx]\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "loaded_data = torch.load('latent_dataset.pt', weights_only=False)\n",
    "latent_dataset = LatentDataset(loaded_data)\n",
    "train_loader = DataLoader(latent_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "vqvae2_model = VQVAE2()\n",
    "initialize_weights(vqvae2_model)\n",
    "train_vqvae2(vqvae2_model, train_loader)\n",
    "\n",
    "\n",
    "def generate_samples(model, num_samples=1, save_path='generated_sample.wav', sampling_rate=24000, output_length=24000):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.randn(1, 1, 8, 4096).to(next(model.parameters()).device)\n",
    "        z = model.encoder(dummy_input)\n",
    "        _, _, latent_height, latent_width = z.shape\n",
    "\n",
    "        encoding_indices = torch.randint(0, model.vq.n_embeddings, (num_samples, latent_height, latent_width)).to(\n",
    "            next(model.parameters()).device)\n",
    "        quantized_latents = model.vq.embeddings(encoding_indices).view(num_samples, model.vq.embedding_dim,\n",
    "                                                                       latent_height, latent_width)\n",
    "\n",
    "        generated_samples = model.decoder(quantized_latents)\n",
    "        print(\"Generated samples shape:\", generated_samples.size())\n",
    "\n",
    "        generated_samples = generated_samples.view(num_samples, -1)\n",
    "        generated_waveform = F.interpolate(generated_samples.unsqueeze(1), size=(output_length,), mode='linear',\n",
    "                                           align_corners=False)\n",
    "        generated_waveform = generated_waveform.squeeze().cpu()\n",
    "        generated_waveform = generated_waveform / torch.max(torch.abs(generated_waveform))\n",
    "        generated_waveform_np = generated_waveform.numpy().astype(np.float32)\n",
    "        torchaudio.save(save_path, torch.tensor(generated_waveform_np).unsqueeze(0), sampling_rate)\n",
    "\n",
    "        print(f\"Sample saved to {save_path}\")\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(generated_waveform_np)\n",
    "        plt.title('Generated Waveform')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.show()\n",
    "\n",
    "        return generated_waveform_np\n",
    "\n",
    "\n",
    "generate_samples(vqvae2_model, output_length=48000)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54ca44419c03ac1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
